# ðŸ“Œ Proyecto E-commerce Analytics (Batch + Streaming)

## 1. ðŸš€ DescripciÃ³n del Proyecto
Este proyecto implementa una **arquitectura moderna de Analytics para E-commerce**, integrando datos histÃ³ricos (batch) y datos en tiempo real (streaming).  

El objetivo es **crear dashboards en Looker Studio** que permitan analizar mÃ©tricas clave del negocio:
- Ingresos (*Revenue*).  
- Ã“rdenes totales y detalle por producto.  
- Clientes Ãºnicos y su distribuciÃ³n geogrÃ¡fica.  
- Ticket promedio (*Average Order Value - AOV*).  
- Comparativa entre batch (histÃ³rico) y streaming (tiempo real).  

---

## 2. ðŸ› ï¸ Servicios de Google Cloud utilizados
| Servicio | Uso en el Proyecto |
|----------|--------------------|
| ![Cloud Storage](./imagenes/cloud_storage.png) | Almacenamiento de los CSV batch y los scripts Python. |
| ![BigQuery](./imagenes/bigquery.png) | Data Warehouse para datos histÃ³ricos, vistas analÃ­ticas y uniÃ³n batch + streaming. |
| ![Pub/Sub](./imagenes/pubsub.png) | Ingesta de eventos en tiempo real (Ã³rdenes simuladas). |
| ![Dataflow](./imagenes/dataflow.png) | Pipeline de streaming que procesa los eventos y los inserta en BigQuery. |
| ![Python](./imagenes/python.png) | Scripts de simulaciÃ³n (`publisher.py`) y pipeline. |
| Looker Studio | Dashboards interactivos para anÃ¡lisis. |

---

## 3. ðŸ—‚ï¸ Modelo Entidad-RelaciÃ³n (ERD)

### Tablas principales
- **Customers**
  - `customer_id (PK)`
  - `name`
  - `country`
  - `signup_date`

- **Orders**
  - `order_id (PK)`
  - `customer_id (FK)`
  - `order_date`

- **OrderItems**
  - `order_id (FK)`
  - `product_id (FK)`
  - `qty`
  - `unit_price`

- **Products**
  - `product_id (PK)`
  - `category`
  - `price`

### Relaciones
- **Customers (1) â†’ (N) Orders**  
- **Orders (1) â†’ (N) OrderItems**  
- **Products (1) â†’ (N) OrderItems**  

ðŸ“Œ **OrderItems es la tabla puente**: conecta Ã³rdenes con productos y permite calcular mÃ©tricas como revenue.  

![Modelo ER](./imagenes/modelo_er.png)  

---
## 4. ðŸ“‚ Pipelines

ðŸ”¹ Pipeline Batch (ETL con BigQuery)

Imagen

ðŸ“Œ Objetivo: cargar los archivos CSV histÃ³ricos desde Cloud Storage a BigQuery y generar la vista de ventas histÃ³ricas (v_fact_sales_batch).

Pasos:

  1 - Subimos los archivos CSV (customers.csv, orders.csv, order_items.csv, products.csv) al bucket bucket-ecommerce-octavio/datasets/.
  2 - Desde BigQuery cargamos esos archivos a tablas dentro del dataset data_ecommerce_demo.
  3 - Creamos la vista de hechos batch:

  ```python
  CREATE OR REPLACE VIEW `data-ecommerce-demo.data_ecommerce_demo.v_fact_sales_batch` AS
  SELECT 
    o.order_id,
    TIMESTAMP(o.order_date) AS ts,  
    o.customer_id,
    oi.product_id,
    (oi.qty * oi.unit_price) AS gross_amount
  FROM `data-ecommerce-demo.data_ecommerce_demo.orders` o
  JOIN `data-ecommerce-demo.data_ecommerce_demo.order_items` oi USING (order_id);
 ```
 ðŸ“Œ Resultado: Vista que consolida ventas histÃ³ricas con detalle de revenue por orden, cliente y producto.

 ðŸ”¹ Pipeline Streaming (Pub/Sub â†’ Dataflow â†’ BigQuery)

 Imagen

 ðŸ“Œ Objetivo: procesar Ã³rdenes simuladas en tiempo real y guardarlas en BigQuery en la tabla fact_sales_streaming.

  Componentes:
  
 -  publisher.py â†’ script que publica eventos simulados en un tÃ³pico de Pub/Sub.
 - Dataflow (Apache Beam) â†’ pipeline que lee los eventos, los transforma y los escribe en BigQuery.
  
  Ejemplo de evento publicado
```python
  {
  "event_id": "123e4567-e89b-12d3-a456-426614174000",
  "order_id": "O1234",
  "customer_id": "C054",
  "product_id": "P002",
  "qty": 2,
  "unit_price": 120.50,
  "event_ts": "2025-09-15 14:23:55"
}
```
CÃ³digo principal del pipeline (simplificado - publisher.py )
```python

import json
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions
from apache_beam.io.gcp.bigquery import WriteToBigQuery

class ParseJson(beam.DoFn):
    def process(self, msg_bytes):
        msg = json.loads(msg_bytes.decode("utf-8"))
        yield {
            "order_id": msg.get("order_id"),
            "customer_id": msg.get("customer_id"),
            "product_id": msg.get("product_id"),
            "gross_amount": float(msg.get("qty",0)) * float(msg.get("unit_price",0)),
            "event_ts": msg.get("event_ts")
        }

def run():
    options = PipelineOptions(
        runner="DataflowRunner",
        project="data-ecommerce-demo",
        region="us-central1",
        temp_location="gs://bucket-ecommerce-octavio/temp",
        staging_location="gs://bucket-ecommerce-octavio/staging",
        job_name="pubsub-to-bigquery-demo"
    )
    options.view_as(StandardOptions).streaming = True

    with beam.Pipeline(options=options) as p:
        (p
         | "ReadPubSub" >> beam.io.ReadFromPubSub(
                subscription="projects/data-ecommerce-demo/subscriptions/order_events-sub-demo")
         | "ParseJSON" >> beam.ParDo(ParseJson())
         | "WriteToBQ" >> WriteToBigQuery(
                table="data-ecommerce-demo.data_ecommerce_demo.fact_sales_streaming",
                schema="order_id:STRING, customer_id:STRING, product_id:STRING, gross_amount:FLOAT, event_ts:TIMESTAMP",
                write_disposition="WRITE_APPEND"
            )
        )

```
ðŸ“Œ Resultado: cada orden publicada en Pub/Sub aparece en tiempo real en BigQuery â†’ tabla

---
## 5. ðŸ“‚ Paso a Paso del Proyecto

### ðŸ”¹ 1. Ingesta en Cloud Storage
Se creÃ³ el bucket **`bucket-ecommerce-octavio`** con:
- Carpeta `/datasets` â†’ CSV histÃ³ricos (customers, orders, order_items, products).  
- Carpeta `/pipelines` â†’ scripts Python:  
  - **publisher.py** â†’ publica eventos simulados en Pub/Sub.  

```python
import json, time, uuid, random
from google.cloud import pubsub_v1

publisher = pubsub_v1.PublisherClient()
topic_path = "projects/data-ecommerce-demo/topics/order_events"

while True:
    event = {
        "event_id": str(uuid.uuid4()),
        "order_id": f"O{random.randint(1000,9999)}",
        "customer_id": f"C{random.randint(1,100)}",
        "product_id": f"P{random.randint(1,50)}",
        "qty": random.randint(1,5),
        "unit_price": round(random.uniform(10,500),2),
        "event_ts": str(time.time())
    }
    publisher.publish(topic_path, json.dumps(event).encode("utf-8"))
    print("Published:", event)
    time.sleep(2)

```
### ðŸ”¹ 2. Carga de datos en BigQuery
Se creÃ³ el dataset data_ecommerce_demo dentro del proyecto data-ecommerce-demo.
Se cargaron las tablas batch desde CSV:

- customers
- orders
- order_items
- products

Ejemplo de consultas exploratorias (batch):

```python
  -- Cantidad de clientes por paÃ­s --
  SELECT country, COUNT(*) AS total_clientes
  FROM `data-ecommerce-demo.data_ecommerce_demo.customers`
  GROUP BY country
  ORDER BY total_clientes DESC;

  -- Clientes por aÃ±o de registro --
  SELECT 
    EXTRACT(YEAR FROM signup_date) AS anio_registro,
    COUNT(*) AS total_clientes
  FROM `data-ecommerce-demo.data_ecommerce_demo.customers`
  GROUP BY anio_registro
  ORDER BY anio_registro;

```
### ðŸ”¹ 3. CreaciÃ³n de vistas para KPIs (Batch)
Ejemplo de vista de ventas histÃ³ricas:

```python
  CREATE OR REPLACE VIEW `data-ecommerce-demo.data_ecommerce_demo.v_fact_sales_batch` AS
  SELECT 
    o.order_id,
    TIMESTAMP(o.order_date) AS ts,  
    o.customer_id,
    oi.product_id,
    (oi.qty * oi.unit_price) AS gross_amount
  FROM `data-ecommerce-demo.data_ecommerce_demo.orders` o
  JOIN `data-ecommerce-demo.data_ecommerce_demo.order_items` oi USING (order_id);


```
