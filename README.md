# ğŸ“Œ Proyecto E-commerce Analytics (Batch + Streaming)

## 1. ğŸš€ DescripciÃ³n del Proyecto
Este proyecto implementa una **arquitectura moderna de Analytics para E-commerce**, integrando datos histÃ³ricos (batch) y datos en tiempo real (streaming).  

El objetivo es **crear dashboards en Looker Studio** que permitan analizar mÃ©tricas clave del negocio:
- Ingresos (*Revenue*).  
- Ã“rdenes totales y detalle por producto.  
- Clientes Ãºnicos y su distribuciÃ³n geogrÃ¡fica.  
- Ticket promedio (*Average Order Value - AOV*).  
- Comparativa entre batch (histÃ³rico) y streaming (tiempo real).  

---

## 2. ğŸ› ï¸ Servicios de Google Cloud utilizados
| Servicio | Uso en el Proyecto |
|----------|--------------------|
|  <div align="center"><img src="Imagenes/cloudstorage.png" width="50" height="50"/></div> | **Cloud Storage**: almacenamiento de los CSV batch y los scripts Python. |
|  <div align="center"><img src="Imagenes/bigquery.png" width="50" height="50"/></div>  | **BigQuery**: Data Warehouse para datos histÃ³ricos, vistas analÃ­ticas y uniÃ³n batch + streaming. |
| <div align="center"><img src="Imagenes/pub.png" width="50" height="50"/></div> | **Pub/Sub**: Ingesta de eventos en tiempo real (Ã³rdenes simuladas). |
| <div align="center"><img src="Imagenes/dataflow.png" width="50" height="50"/></div>  | **DataFlow**: Pipeline de streaming que procesa los eventos y los inserta en BigQuery. |
| <div align="center"><img src="Imagenes/python.png" width="50" height="50"/></div> | **Python**: Scripts de simulaciÃ³n (`publisher.py`) y pipeline. |
| <div align="center"><img src="Imagenes/looker.png" width="50" height="50"/></div> | **Looker Studio**: Dashboards interactivos para anÃ¡lisis. |

---

## 3. ğŸ—‚ï¸ Modelo Entidad-RelaciÃ³n (ERD)

<div align="center"><img src="Imagenes/MER.png" width="600" /></div>

### Tablas principales
- **Customers**
  - `customer_id (PK)`
  - `name`
  - `country`
  - `signup_date`

- **Orders**
  - `order_id (PK)`
  - `customer_id (FK)`
  - `order_date`

- **OrderItems**
  - `order_id (FK)`
  - `product_id (FK)`
  - `qty`
  - `unit_price`

- **Products**
  - `product_id (PK)`
  - `category`
  - `price`

### Relaciones
- **Customers (1) â†’ (N) Orders**  
- **Orders (1) â†’ (N) OrderItems**  
- **Products (1) â†’ (N) OrderItems**  

ğŸ“Œ **OrderItems es la tabla puente**: conecta Ã³rdenes con productos y permite calcular mÃ©tricas como revenue.  



---
## 4. ğŸ“‚ Pipelines

ğŸ”¹ Pipeline Batch (ETL con BigQuery)

<div align="center"><img src="Imagenes/batch.png" width="2419" height="798"/></div>

ğŸ“Œ Objetivo: cargar los archivos CSV histÃ³ricos desde Cloud Storage a BigQuery y generar la vista de ventas histÃ³ricas (v_fact_sales_batch).

Pasos:
  1 - Creo el bucket ecommerce-demo-bucket en Cloud Storage con las carpetas datasets y pipelines
  2 - Subo los archivos CSV (customers.csv, orders.csv, order_items.csv, products.csv) al bucket ecommerce-demo-bucker/datasets/.
  <br>
  2 - Desde BigQuery cargo esos archivos a tablas dentro del dataset data_ecommerce_demo.
  <br>
  4 - Creamos la vista de hechos batch:

  ```python
  CREATE OR REPLACE VIEW `data-ecommerce-demo.data_ecommerce_demo.v_fact_sales_batch` AS
  SELECT 
    o.order_id,
    TIMESTAMP(o.order_date) AS ts,  
    o.customer_id,
    oi.product_id,
    (oi.qty * oi.unit_price) AS gross_amount
  FROM `data-ecommerce-demo.data_ecommerce_demo.orders` o
  JOIN `data-ecommerce-demo.data_ecommerce_demo.order_items` oi USING (order_id);
 ```
 ğŸ“Œ Resultado: Vista que consolida ventas histÃ³ricas con detalle de revenue por orden, cliente y producto.
 
---
 ğŸ”¹ Pipeline Streaming (Pub/Sub â†’ Dataflow â†’ BigQuery)

 <div align="center"><img src="Imagenes/Streaming.png"/></div>

 ğŸ“Œ Objetivo: procesar Ã³rdenes simuladas en tiempo real y guardarlas en BigQuery en la tabla fact_sales_streaming.

  Componentes:
  
 - Archivo publisher.py â†’ script en Python que publica eventos simulados en un Tema de Pub/Sub.
 - Dataflow (Apache Beam) â†’ pipeline que lee los eventos, los transforma y los escribe en BigQuery.
  
  Ejemplo de evento publicado
```python
  {
  "event_id": "123e4567-e89b-12d3-a456-426614174000",
  "order_id": "O1234",
  "customer_id": "C054",
  "product_id": "P002",
  "qty": 2,
  "unit_price": 120.50,
  "event_ts": "2025-09-15 14:23:55"
}
```
Vista que unifica batch y streaming
```python

-- Vista que combina batch con streaming para ser utilizada en Looker Studio --

CREATE OR REPLACE VIEW `data-ecommerce-demo.data_ecommerce_demo.v_fact_sales_all` AS
-- HistÃ³rico (batch)
SELECT 
  o.order_id,
  TIMESTAMP(o.order_date) AS ts,  
  o.customer_id,
  oi.product_id,
  (oi.qty * oi.unit_price) AS gross_amount,
  'batch' AS source
FROM `data-ecommerce-demo.data_ecommerce_demo.orders` o
JOIN `data-ecommerce-demo.data_ecommerce_demo.order_items` oi USING (order_id)

UNION ALL

-- Streaming (tiempo real)
SELECT 
  order_id,
  event_ts AS ts,
  customer_id,
  product_id,
  gross_amount,
  'streaming' AS source
FROM `data-ecommerce-demo.data_ecommerce_demo.fact_sales_streaming`;

```
ğŸ“Œ Resultado: cada orden publicada en Pub/Sub aparece en tiempo real en BigQuery â†’ tabla

---

### ğŸ”¹ 4. Dashboard Batch

<div align="center"><img src="Imagenes/batchDashboard.png"/></div>

**KPIs principales:**

  - Total Revenue
  - Total Orders
  - Unique Clients
  - Average Order Value (AOV)

**GrÃ¡ficos:**

  - Revenue a lo largo del tiempo â†’ crecimiento acumulado.
  - Revenue por categorÃ­a â†’ distribuciÃ³n entre Books, Clothing, Electronics, etc.
  - Revenue por cliente/paÃ­s/categorÃ­a â†’ tabla de detalle.
  - Revenue por paÃ­s â†’ mapa geogrÃ¡fico.

ğŸ“Œ Insights:

  - Chile y Argentina concentran la mayorÃ­a de clientes.
  - Electronics y Books son las categorÃ­as mÃ¡s rentables.
  - Ticket promedio (AOV): $1.465,68.

### ğŸ”¹ 4. Dashboard Streaming
